# Yelp_Analysis

This semester, I developed a machine learning model that predicted star ratings based on a user’s overall Yelp review. 

To begin preprocessing, I used Databricks, which is a cloud-based engineering tool that helps users transform large amounts of data. I read in three main JSON files from the Yelp Open dataset that are related to my end goal - business.json, reviews.json, and user.json. I started the process by reading in each of the three JSON files to their own respective dataframe and performing Data Cleaning features on each - dropping null values and removing unnecessary columns. I then join them all together to create ‘yelp_sdf’. I then write this dataframe out as a parquet file to my raw folder in my Amazon S3 bucket. 

Afterwards, I began to do Feature Engineering. Categorical variables underwent encoding using `StringIndexer` for indexing and `OneHotEncoder` for one-hot encoding. These transformations convert categorical data into a numerical format, which is necessary when working with machine learning algorithms. Text data underwent a series of processing steps as well. This included tokenization, hashing, and TF-IDF processing, all of which derived features from the textual content of the reviews. Afterwards, I assembled each vector using VectorAssembler so that they would all become one singular, large vector. This vector would be in a column called ‘features’.

Additionally, I chose to make use of a ‘Binarizer’ to create a binary label indicating whether a review's star rating was greater than 3 or not. This transformation allows for the transition from a regression task to a binary classification problem.

For the predictive modeling step, I chose to use a logistic regression algorithm, which was suitable for binary classification tasks. The logistic regression model was then integrated into my pipeline. Then, I split my data into training and testing datasets, which are intended to assess the model's generalization performance.



To evaluate the model's effectiveness, I used a binary classification tool called Area Under the ROC Curve (AUC). The Area Under The Curve measures the model’s ability to distinguish between classes. The AUC value ranges from 0 to 1, with a higher value indicating better performance. An AUC of 0.885 indicates that our model has a very high discriminative ability. The final model of AUC was saved for future use and the project maintained a consistent approach to binary classification throughout the entire workflow. 

Next, I created a confusion matrix to show the number of true positives, true negatives, false positives, and false negatives. The number of prediction positives and actual true positives are 4,946, which is the largest value here. Since the number of true positives is higher than the rest, it indicates that the model has successfully identified a substantial proportion of instances where the positive class is true, showcasing the model's effectiveness in correctly predicting positive outcomes. This high number of true positives suggests that the model has a strong ability to recognize and classify instances belonging to the positive class.

Additional model metrics were generated as well. First, we found out the accuracy, which is the ratio of correctly predicted instances to the total instances in the dataset. Our model produced an accuracy score of 0.854, or about 85%, which indicates that the model correctly predicted the star rating for about 85% of the instances in the dataset. Next, we found the precision, which is the ratio of correctly predicted positive observations (total true positives) to the total predicted positives (total of both true positives and false positives). Our model produced a precision score of about 0.913, or about 91%, which indicates that the model is correct about 91% of the time it predicts a star rating. Next, we found our recall score, which is the ratio of correctly predicted positive observations to the total true positives. Our model produced a recall score of 0.895, or about 90%, which indicates that the model correctly identified and predicted about 90% of the true positive instances. Lastly, we found our F1 score, which is a balanced combination of the precision and recall scores. Our model produced a F1 score of 0.904, or about 90%, which indicates a good balance between our precision and recall scores.

Lastly, I created visualizations depicting these metrics. I created a Receiver Operating Curve (ROC) and a Precision-Recall Curve. The ROC Curve, as mentioned earlier, measures the model’s ability to distinguish between classes. Our ROC Curve had an AUC of 0.885, which reflected high discriminative ability. My Precision-Recall curve was high along the y-axis, indicating that it has precision across different levels of recall and maintains high precision even when recall increases. This is a favorable scenario, as it means the model is capable of providing accurate positive predictions while still capturing a good portion of the actual positive instances. Both of these models were saved back to my Amazon S3 buckets.

